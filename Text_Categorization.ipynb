{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Spark\n",
        "\n",
        "Import the required libraries and initialize spark-session using different configuration parameters. The configuration values depend upon my local environment. Adjust the parameters accordingly."
      ],
      "metadata": {
        "id": "E8Hluxvy-FIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YQ_-08if-P8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvGiOYt--EUo"
      },
      "outputs": [],
      "source": [
        "# Import Spark NLP\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline# Start Spark Session with Spark NLP\n",
        "#spark = sparknlp.start()spark = SparkSession.builder \\\n",
        "    .appName(\"BBC Text Categorization\")\\\n",
        "    .config(\"spark.driver.memory\",\"8G\")\\ change accordingly\n",
        "    .config(\"spark.memory.offHeap.enabled\",True)\\\n",
        "    .config(\"spark.memory.offHeap.size\",\"8G\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\")\\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
        "    .config(\"spark.network.timeout\",\"3600s\")\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Text Data\n",
        "\n",
        "We will be using BBC data. You can download the data from this [link text](https://www.kaggle.com/code/yufengdev/bbc-text-categorization/notebook). After downloading the data, load the data in spark using below code;"
      ],
      "metadata": {
        "id": "Lz4RJuqc-SBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File location and type\n",
        "file_location = r'path\\to\\bbc-text.csv'\n",
        "file_type = \"csv\"# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)df.count()"
      ],
      "metadata": {
        "id": "ntTLmELx-bbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into train and test sets\n",
        "\n",
        "Spark Dataframe has an inbuilt function called randomSplit() to perform the same operation."
      ],
      "metadata": {
        "id": "nOPyDXNl-l2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\n",
        "# The randomSplit() function requires 2 parameters viz. weights array and seed. \n",
        "# For our example, we will be using 70–30 split where 70% will be the training \n",
        "# data and 30% will be the test data."
      ],
      "metadata": {
        "id": "S0RVMTL_-sPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s go ahead and build the NLP pipeline using Spark NLP. One of the biggest advantages of Spark NLP is that it natively integrates with Spark MLLib modules that help to build a comprehensive ML pipeline consisting of transformers and estimators. This pipeline can include feature extraction modules like CountVectorizer or HashingTF and IDF. We can also include a machine learning model in this pipeline. Below is the example consisting of the NLP pipeline with feature extraction and machine learning model;"
      ],
      "metadata": {
        "id": "W7cYI4xDDU4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer,IndexToString\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator# convert text column to spark nlp document\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")# convert document to array of tokens\n",
        "tokenizer = Tokenizer() \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"token\")\n",
        " \n",
        "# clean tokens \n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")# remove stopwords\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)# stems tokens to bring it to root form\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"stem\")# Convert custom document structure to array of tokens.\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"stem\"]) \\\n",
        "    .setOutputCols([\"token_features\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(False)# To generate Term Frequency\n",
        "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\", numFeatures=1000)# To generate Inverse Document Frequency\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)# convert labels (string) to integers. Easy to process compared to string.\n",
        "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")# define a simple Multinomial logistic regression model. Try different combination of hyperparameters and see what suits your data. You can also try different algorithms and compare the scores.\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.0)# To convert index(integer) to corresponding class labels\n",
        "label_to_stringIdx = IndexToString(inputCol=\"label\", outputCol=\"article_class\")# define the nlp pipeline\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[document_assembler, \n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            stemmer, \n",
        "            finisher,\n",
        "            hashingTF,\n",
        "            idf,\n",
        "            label_stringIdx,\n",
        "            lr,\n",
        "            label_to_stringIdx])"
      ],
      "metadata": {
        "id": "Ewa1l86TDVQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "oen6KgZ3Dgt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the pipeline on training data\n",
        "pipeline_model = nlp_pipeline.fit(trainingData)\n"
      ],
      "metadata": {
        "id": "cBIFU0kUDhFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform predictions"
      ],
      "metadata": {
        "id": "KyWD0HEsDkjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform predictions on test data\n",
        "predictions =  pipeline_model.transform(testData)"
      ],
      "metadata": {
        "id": "eFx6ub8hDnRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model"
      ],
      "metadata": {
        "id": "nw-0tuL9DqlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import evaluator\n",
        "# Accuracy\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluatorevaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = %g\" % (accuracy))\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))"
      ],
      "metadata": {
        "id": "z8eqrrz2Dr5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = %g\" % (accuracy))\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))"
      ],
      "metadata": {
        "id": "T43ayNshDyRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy = %g\" % (accuracy))\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))"
      ],
      "metadata": {
        "id": "VegwwRa5D3ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the Pipeline Model"
      ],
      "metadata": {
        "id": "5s8tCf-7D7g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_model.save('/path/to/storage_location')"
      ],
      "metadata": {
        "id": "6G6XN2NgD-FV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}